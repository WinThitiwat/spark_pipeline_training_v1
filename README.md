## Spark Pipeline Training [WORK IN PROGRESS]

## TL;DR
This training project is an E2E data pipeline to improve the big data domain knowledge, which includes
 1. extract data from CSV, parquet file. Note the existing CSV is a sample file
 2. process the data using PySpark 
 3. load data into the Hadoop cluster, Hive data warehouse, and data lake S3

## Project Structure
TBA

## Set up
### Local env
TBA

### Hadoop on GCP Compute Engineer 
TBA

## How to run
- Navigate to the `spark_pipeline_training/src/main/python/bin`
  > cd ~/spark_pipeline_training/src/main/python/bin
- Trigger the pipeline's main function
  > python3 run_presc_pipeline.py

## References
TBA
